{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PrimateFace Tutorial: Automated Video Timestamping\n",
        "\n",
        "| GitHub | Paper | Website |\n",
        "|---|---|---|\n",
        "| [Code](https://github.com/KordingLab/PrimateFace) | [Preprint](https://www.biorxiv.org/content/10.1101/2025.08.12.669927v2) | [Project](https://primateface.studio/) |\n",
        "\n",
        "Welcome! This tutorial notebook walks through using a **PrimateFace** detection model to automatically find and timestamp primate faces in videos, creating a 'visibility baseline' for behavioral coding.\n",
        "\n",
        "### This notebook will:\n",
        "1. **Set up the environment** to run _mmdetection_ face detection models trained on PrimateFace data.\n",
        "2. **Download PrimateFace model**\n",
        "3. **Load video data** from local path or Google Drive link (default).\n",
        "4. **Calibrate Detection Confidence**: Use an interactive tool to find the optimal face detection sensitivity for your video.\n",
        "5. **Analyze Video**: Apply this setting to process the entire video and log all face detections.\n",
        "6. **Visualize Results**: Create a timeline of face visibility.\n",
        "7. **Export to BORIS**: Automatically convert the results into a `.boris` project file with \"face present\" events pre-coded.\n",
        "\n",
        "---\n",
        "### **Quick Start Instructions**\n",
        "\n",
        "*   **Set Your Runtime to GPU**: Go to **Runtime > Change runtime type > T4 GPU**. This is essential for performance.\n",
        "*   **Run Cells Sequentially**: Click the \"Play\" button on each cell to run it. The first few cells handle setup and may take a few moments. Do not 'Run All'\n",
        " so that you don't skip over the interactive threshold picker.\n",
        "*   **Restarting**: Installiing the dependencies requires restarting the Runtime. You can do this by clicking the \"Runtime\" menu and selecting \"Restart session\".\n",
        "\n",
        "This is a demo notebook and extending this to your own videos requires setting up the environment, GPU access, and paths to your own data.\n",
        "\n",
        "Let's begin!"
      ],
      "metadata": {
        "id": "aW2a7plZ9JK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Set up the environment (this will take a couple of min)**"
      ],
      "metadata": {
        "id": "u86OBIKkbRVQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfmqD3GY9CKH",
        "outputId": "f9e3bbf8-dd9d-4d4d-a094-cd917c1e0150",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug 19 20:40:55 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#@title Check GPU availability. If not available,, change Runtime type to 'T4 GPU'\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PyTorch, mmdet, mmpose, and Insightface with strict numpy pinning\n",
        "%%capture\n",
        "# 1. Clean slate\n",
        "!pip uninstall -y numpy xtcocotools pycocotools fastai spacy thinc pymc pytensor jax jaxlib yfinance\n",
        "\n",
        "# 2. Install numpy FIRST with --force-reinstall\n",
        "!pip install --force-reinstall --no-deps numpy==1.26.4\n",
        "\n",
        "# 3. Install torch stack (won't touch numpy)\n",
        "!pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# 4. Install other deps with numpy constraint\n",
        "!pip install --no-cache-dir \"opencv-python-headless<4.9\" moviepy==1.0.3 imageio imageio-ffmpeg \"numpy==1.26.4\"\n",
        "\n",
        "# 5. Install MMDet/MMPose deps with constraints\n",
        "!pip install -U openmim \"numpy==1.26.4\"\n",
        "!mim install \"mmengine==0.10.3\" \"numpy==1.26.4\"\n",
        "!mim install \"mmcv==2.1.0\" \"numpy==1.26.4\"\n",
        "\n",
        "# 6. Clone and install mmpose\n",
        "!rm -rf mmpose\n",
        "!git clone https://github.com/open-mmlab/mmpose.git\n",
        "%cd mmpose\n",
        "!pip install -e . --no-deps\n",
        "!pip install -r requirements.txt \"numpy==1.26.4\"\n",
        "%cd ..\n",
        "\n",
        "# 7. Install mmdet with constraint\n",
        "!pip install mmdet==3.3.0 \"numpy==1.26.4\"\n",
        "\n",
        "# 8. Build xtcocotools from source against correct numpy\n",
        "!pip install cython\n",
        "!pip install --no-binary :all: xtcocotools\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-mh_aK2lT1ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#@title 1.3 Install detection dependencies\n",
        "%pip install -U openmim\n",
        "!mim install \"mmengine==0.10.3\"\n",
        "!mim install \"mmcv==2.1.0\"\n",
        "\n",
        "# Install mmdetection\n",
        "!rm -rf mmdetection\n",
        "!git clone https://github.com/open-mmlab/mmdetection.git\n",
        "%cd mmdetection\n",
        "\n",
        "%pip install -e .\n",
        "%pip install -q lap\n",
        "%pip install -q mmpose"
      ],
      "metadata": {
        "id": "8N6oWdvq-eHJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now '**restart**' the session.\n",
        "\n",
        "To fully use the newly installed packages, we need to restart the colab notebook runtime.\n",
        "\n",
        "1.  Click **Runtime** > **Restart session**\n",
        "2.  Rerun the setup cells above.\n",
        "3.  Continue with the rest of the notebook."
      ],
      "metadata": {
        "id": "45OncaCeo-9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.4 Import the necessary libraries for timestamping analysis.\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import gdown\n",
        "import time\n",
        "from pathlib import Path\n",
        "import argparse\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import copy\n",
        "from typing import List, Dict, Optional, Any, Tuple\n",
        "import random\n",
        "import datetime\n",
        "import tempfile\n",
        "\n",
        "# --- MMDetection & MMPose ---\n",
        "# These are the core libraries for running the deep learning models.\n",
        "from mmdet.apis import inference_detector, init_detector\n",
        "from mmengine.structures import InstanceData\n",
        "from mmdet.structures import DetDataSample\n",
        "from mmpose.evaluation.functional import nms # Using nms from mmpose as in gradio script\n",
        "from mmdet.models.trackers import ByteTracker # Import ByteTracker\n",
        "from mmengine.registry import MODELS as MMENGINE_MODELS # If building tracker from dict\n",
        "\n",
        "# --- Visualization ---\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# --- Interactive Widgets ---\n",
        "from ipywidgets import interact, FloatSlider\n",
        "from IPython.display import display, clear_output\n",
        "_INTERACTIVE_WIDGETS_AVAILABLE = True\n",
        "\n",
        "# --- Warnings ---\n",
        "# Suppress common warnings from the detection libraries for a cleaner output.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
      ],
      "metadata": {
        "id": "mQDLXDbd-wAs",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "e8a1f192-4aaf-4356-cd17-618607aa7f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mmcv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-500414106.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# --- MMDetection & MMPose ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# These are the core libraries for running the deep learning models.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmmdet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minference_detector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_detector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmmengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstanceData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmmdet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetDataSample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mmdetection/mmdet/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Copyright (c) OpenMMLab. All rights reserved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmmcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmmengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmmengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdigit_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mmcv'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Download PrimateFace and set up helper functions**\n",
        "Here, we'll download one of PrimateFace face detection models, which includes:\n",
        " 1. the model configuration file and\n",
        " 2. the model checkpoint (AKA weights).\n",
        "\n",
        "We'll download these files directly from a Google Drive link.\n"
      ],
      "metadata": {
        "id": "zbSRc9s3X8yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.1 Download PrimateFace cascade-rcnn detection model\n",
        "\n",
        "\n",
        "# --- Action Required: Paste your Google Drive shareable links here ---\n",
        "CONFIG_GDRIVE_LINK = \"https://drive.google.com/file/d/1Y_YFdIDRcWQLI-gRiCnOrDxCptzCiiNp/view?usp=drive_link\"\n",
        "WEIGHTS_GDRIVE_LINK = \"https://drive.google.com/file/d/1zZ8S31zPHX5BWYKbnHxI1QOqP-fPnVFO/view?usp=drive_link\"\n",
        "\n",
        "# --- Define local filenames for the downloaded files ---\n",
        "downloaded_config_path = \"downloaded_model_config.py\"\n",
        "downloaded_weights_path = \"downloaded_model_weights.pth\"\n",
        "\n",
        "# --- Download function ---\n",
        "def download_file_from_gdrive(url, output_path):\n",
        "    \"\"\"Uses the gdown library to download a file from a Google Drive link.\"\"\"\n",
        "    print(f\"Attempting to download from Google Drive to {output_path}...\")\n",
        "    try:\n",
        "        gdown.download(url, output_path, quiet=False, fuzzy=True)\n",
        "        if os.path.exists(output_path):\n",
        "            print(f\"Successfully downloaded: {output_path}\\n\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"Download failed. File not found at {output_path}. Check your share link and permissions.\\n\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during download: {e}\\n\")\n",
        "        return False\n",
        "\n",
        "# --- Execute the downloads ---\n",
        "config_download_success = download_file_from_gdrive(CONFIG_GDRIVE_LINK, downloaded_config_path)\n",
        "weights_download_success = download_file_from_gdrive(WEIGHTS_GDRIVE_LINK, downloaded_weights_path)\n",
        "\n",
        "# --- IMPORTANT: Update the main configuration paths ---\n",
        "# This tells the rest of the notebook to use the files we just downloaded.\n",
        "if config_download_success:\n",
        "    MMDET_CONFIG_PATH = downloaded_config_path\n",
        "if weights_download_success:\n",
        "    MMDET_CHECKPOINT_PATH = downloaded_weights_path\n",
        "\n",
        "\n",
        "MMDET_DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEFAULT_NMS_THRESHOLD = 0.3\n",
        "\n",
        "\n",
        "print(\"--- Path Update Summary ---\")\n",
        "print(f\"Model Config Path is now set to: {MMDET_CONFIG_PATH}\")\n",
        "print(f\"Model Checkpoint Path is now set to: {MMDET_CHECKPOINT_PATH}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uhUttwghcdZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup for multi-animal tracker\n",
        "\n",
        "# ByteTracker configuration\n",
        "# These parameters can be tuned for different tracking performance.\n",
        "DEFAULT_TRACKER_CONFIG = dict(\n",
        "    motion=dict(type='KalmanFilter'),\n",
        "    obj_score_thrs=dict(high=0.6, low=0.1),\n",
        "    init_track_thr=0.7,\n",
        "    weight_iou_with_det_scores=True,\n",
        "    match_iou_thrs=dict(high=0.1, low=0.5, tentative=0.3),\n",
        "    num_frames_retain=30\n",
        ")\n",
        "\n",
        "# Threshold for a detection to be considered by the tracker\n",
        "# This can be lower than the final confidence threshold.\n",
        "DEFAULT_TRACK_CONF_THRESHOLD = 0.4\n",
        "\n",
        "# The maximum number of individuals to assign stable IDs to.\n",
        "DEFAULT_NUM_TRACKED_IDS = 5\n",
        "\n",
        "# Colors for visualizing different track IDs\n",
        "ID_COLORS = {\n",
        "    1: (0, 255, 0),   # Green\n",
        "    2: (0, 0, 255),   # Red\n",
        "    3: (255, 0, 0),   # Blue\n",
        "    4: (255, 255, 0), # Yellow\n",
        "    5: (0, 255, 255), # Cyan\n",
        "    6: (255, 0, 255)  # Magenta\n",
        "}\n",
        "DEFAULT_COLOR = (128, 128, 128) # Gray for unassigned/overflow\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZZKifunQ12Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions for analysis, visualization, and BORIS export.\n",
        "\n",
        "mmdet_model = None\n",
        "\n",
        "def get_model_class_names(model) -> tuple:\n",
        "    \"\"\"Robustly gets class names from a model, handling str, list, or tuple.\"\"\"\n",
        "    meta = model.dataset_meta\n",
        "    classes = meta.get('classes', None)\n",
        "    if classes is None:\n",
        "        return ()\n",
        "    if isinstance(classes, str):\n",
        "        # If it's a single string, wrap it in a tuple\n",
        "        return (classes,)\n",
        "    return tuple(classes)\n",
        "\n",
        "# --- Model Loading ---\n",
        "def load_mmdetection_model(config_path: str, checkpoint_path: str, device: str):\n",
        "    \"\"\"Loads the MMDetection model into memory.\"\"\"\n",
        "    global mmdet_model\n",
        "    if mmdet_model is not None:\n",
        "        print(\"MMDetection model already loaded.\")\n",
        "        return mmdet_model\n",
        "    config_file, checkpoint_file = Path(config_path), Path(checkpoint_path)\n",
        "    if not config_file.exists() or not checkpoint_file.exists():\n",
        "        raise FileNotFoundError(f\"Config or checkpoint not found. Check paths.\")\n",
        "    try:\n",
        "        print(f\"Loading MMDetection model on device: {device}...\")\n",
        "        model = init_detector(str(config_file), str(checkpoint_file), device=device)\n",
        "        print(\"MMDetection model loaded successfully.\")\n",
        "        mmdet_model = model\n",
        "        return mmdet_model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading MMDetection model: {e}\")\n",
        "        raise\n",
        "# --- Video Path Utility ---\n",
        "def get_video_paths(input_path: str) -> List[str]:\n",
        "    \"\"\"Finds all video files in a given path (file or directory).\"\"\"\n",
        "    input_p = Path(input_path)\n",
        "    video_paths = []\n",
        "    video_extensions = [\".mp4\", \".avi\", \".mov\", \".mkv\", \".webm\", \".mts\"]\n",
        "    if input_p.is_file() and input_p.suffix.lower() in video_extensions:\n",
        "        video_paths.append(str(input_p))\n",
        "    elif input_p.is_dir():\n",
        "        for item in input_p.iterdir():\n",
        "            if item.is_file() and item.suffix.lower() in video_extensions:\n",
        "                video_paths.append(str(item))\n",
        "    if not video_paths:\n",
        "        raise FileNotFoundError(f\"No video files found at {input_path}\")\n",
        "    return video_paths\n",
        "\n",
        "# --- Detection Processing ---\n",
        "def filter_and_nms_raw_detections(mm_results_raw: DetDataSample, model_classes: List, confidence_threshold: float, nms_threshold: float, device: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Filters raw model predictions by confidence and applies Non-Maximum Suppression.\"\"\"\n",
        "    processed_detections = []\n",
        "    if not hasattr(mm_results_raw, 'pred_instances') or len(mm_results_raw.pred_instances) == 0:\n",
        "        return []\n",
        "    pred_instances = mm_results_raw.pred_instances\n",
        "    keep_conf = pred_instances.scores >= confidence_threshold\n",
        "    final_instances = pred_instances[keep_conf]\n",
        "    if len(final_instances) == 0:\n",
        "        return []\n",
        "\n",
        "    nms_input = np.hstack((final_instances.bboxes.cpu().numpy(), final_instances.scores.cpu().numpy()[:, np.newaxis]))\n",
        "    keep_nms_indices = nms(nms_input, nms_threshold)\n",
        "\n",
        "    for i in keep_nms_indices:\n",
        "        label_idx = int(final_instances.labels[i].item())\n",
        "        processed_detections.append({\n",
        "            \"bbox\": final_instances.bboxes[i].cpu().numpy().tolist(),\n",
        "            \"score\": float(final_instances.scores[i].item()),\n",
        "            \"label\": model_classes[label_idx]\n",
        "        })\n",
        "    return processed_detections\n",
        "\n",
        "def create_downsampled_video(input_path: str, target_height: int) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Creates a temporary, downsampled version of a video file.\n",
        "\n",
        "    Args:\n",
        "        input_path: Path to the original video.\n",
        "        target_height: The desired height of the new video (e.g., 720).\n",
        "                       Width is scaled automatically to maintain aspect ratio.\n",
        "\n",
        "    Returns:\n",
        "        The file path to the temporary, downsampled video, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    if target_height <= 0:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        print(f\"Downsampling '{Path(input_path).name}' to {target_height}p height...\")\n",
        "        # Create a temporary file that will be deleted on close\n",
        "        temp_video_file = tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False)\n",
        "        temp_video_path = temp_video_file.name\n",
        "\n",
        "        cap = cv2.VideoCapture(input_path)\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Error: Could not open original video {input_path}\")\n",
        "            return None\n",
        "\n",
        "        # Get original video properties\n",
        "        original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "        # If video is already smaller, no need to downsample\n",
        "        if original_height <= target_height:\n",
        "            print(\"Video is already at or below target resolution. No downsampling needed.\")\n",
        "            temp_video_file.close() # It was created, so close it\n",
        "            os.unlink(temp_video_path) # and delete it\n",
        "            return None\n",
        "\n",
        "        # Calculate new width to maintain aspect ratio\n",
        "        aspect_ratio = original_width / original_height\n",
        "        new_width = int(target_height * aspect_ratio)\n",
        "\n",
        "        # Ensure width is even, as required by many video codecs\n",
        "        if new_width % 2 != 0:\n",
        "            new_width += 1\n",
        "\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        writer = cv2.VideoWriter(temp_video_path, fourcc, fps, (new_width, target_height))\n",
        "        if not writer.isOpened():\n",
        "            print(\"Error: Could not initialize VideoWriter.\")\n",
        "            return None\n",
        "\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        for _ in tqdm(range(total_frames), desc=\"Resizing frames\"):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            resized_frame = cv2.resize(frame, (new_width, target_height), interpolation=cv2.INTER_AREA)\n",
        "            writer.write(resized_frame)\n",
        "\n",
        "        cap.release()\n",
        "        writer.release()\n",
        "        print(\"Downsampling complete.\")\n",
        "        return temp_video_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during downsampling: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Visualization ---\n",
        "def visualize_frame_with_detections(frame: np.ndarray, detections: List[Dict[str, Any]], thickness: int = 2):\n",
        "    \"\"\"Draws bounding boxes on a frame, colored by stable ID.\"\"\"\n",
        "    vis_frame = frame.copy()\n",
        "    for det in detections:\n",
        "        x1, y1, x2, y2 = map(int, det[\"bbox\"])\n",
        "        score = det[\"score\"]\n",
        "        label = det[\"label\"]\n",
        "        stable_id = det.get(\"stable_id\")\n",
        "\n",
        "        color = ID_COLORS.get(stable_id, DEFAULT_COLOR) if stable_id is not None else (0, 255, 0)\n",
        "\n",
        "        id_text = f\"ID:{stable_id} \" if stable_id is not None else \"\"\n",
        "\n",
        "        cv2.rectangle(vis_frame, (x1, y1), (x2, y2), color, thickness)\n",
        "\n",
        "        label_text = f\"{id_text}{label}: {score:.2f}\"\n",
        "\n",
        "        label_y_pos = y1 - 10 if y1 > 20 else y1 + 15\n",
        "        cv2.putText(vis_frame, label_text, (x1, label_y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, thickness)\n",
        "    return vis_frame\n",
        "\n",
        "# --- Output Saving ---\n",
        "def save_results_to_json(all_video_data: Dict[str, Any], output_filepath: str):\n",
        "    \"\"\"Saves the detailed analysis to a JSON file.\"\"\"\n",
        "    Path(output_filepath).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(output_filepath, 'w') as f:\n",
        "        json.dump(all_video_data, f, indent=4)\n",
        "    print(f\"Results successfully saved to {output_filepath}\")\n",
        "\n",
        "\n",
        "def save_boris_project(master_results: Dict[str, Any], output_dir: str):\n",
        "    \"\"\"Converts analysis results with tracking into BORIS project files.\"\"\"\n",
        "    if not master_results.get(\"videos\"): return\n",
        "\n",
        "    behavior_code = \"face present\"\n",
        "    behavior_conf = {\"1\": {\"key\": \"1\", \"code\": behavior_code, \"type\": \"STATE\", \"description\": \"Auto-detected\", \"modifiers\": {}, \"category\": \"presence\"}}\n",
        "\n",
        "    for video_data in master_results[\"videos\"]:\n",
        "        video_path = Path(video_data[\"video_filepath\"])\n",
        "        fps, total_frames = video_data[\"fps\"], video_data[\"total_frames\"]\n",
        "        timestamps_by_id = video_data.get(\"timestamps_summary_by_id\", {})\n",
        "\n",
        "        # Determine which subjects are present in this video\n",
        "        present_sids = [sid for sid, ts in timestamps_by_id.items() if ts]\n",
        "        if not present_sids:\n",
        "            print(f\"No tracked detections in {video_path.name}, skipping BORIS file.\")\n",
        "            continue\n",
        "\n",
        "        # Define subjects based on detected IDs for this video\n",
        "        subjects_conf = {str(i): {\"key\": str(i+1), \"name\": f\"primate_{sid}\", \"description\": \"\"} for i, sid in enumerate(present_sids)}\n",
        "        subject_name_map = {sid: f\"primate_{sid}\" for sid in present_sids}\n",
        "\n",
        "        all_events = []\n",
        "        for sid, subject_name in subject_name_map.items():\n",
        "            timestamps = timestamps_by_id.get(sid, [])\n",
        "            if not timestamps: continue\n",
        "\n",
        "            events_for_subject, in_event = [], False\n",
        "            frame_duration = 1.0 / fps\n",
        "            for i, ts in enumerate(timestamps):\n",
        "                if not in_event:\n",
        "                    events_for_subject.append([ts, subject_name, behavior_code, \"\", \"\", \"START\"])\n",
        "                    in_event = True\n",
        "\n",
        "                is_last = (i == len(timestamps) - 1)\n",
        "                if is_last or (timestamps[i+1] - ts) > (frame_duration * 1.5):\n",
        "                    events_for_subject.append([ts + frame_duration, subject_name, behavior_code, \"\", \"\", \"STOP\"])\n",
        "                    in_event = False\n",
        "            all_events.extend(events_for_subject)\n",
        "\n",
        "        all_events.sort(key=lambda x: (x[0], 0 if x[5] == \"START\" else 1))\n",
        "\n",
        "        video_path_str = str(video_path)\n",
        "        observation_data = {\n",
        "            \"date\": datetime.datetime.now().isoformat(), \"description\": \"Auto-generated from script\", \"type\": \"MEDIA\",\n",
        "            \"file\": {\"1\": [video_path_str]},\n",
        "            \"media_info\": {\"length\": {video_path_str: total_frames / fps}, \"fps\": {video_path_str: fps}, \"hasVideo\": {video_path_str: True}, \"hasAudio\": {video_path_str: True}, \"offset\": {\"1\": 0.0}},\n",
        "            \"time offset\": 0.0, \"events\": all_events, \"independent_variables\": {}, \"close_behaviors_between_videos\": False,\n",
        "        }\n",
        "\n",
        "        boris_project = {\n",
        "            \"project_name\": video_path.stem, \"project_date\": datetime.datetime.now().isoformat(),\n",
        "            \"subjects_conf\": subjects_conf, \"behaviors_conf\": behavior_conf,\n",
        "            \"behavioral_categories\": [\"presence\"], \"observations\": {video_path.stem: observation_data},\n",
        "            \"project_format_version\": \"7.0\", \"time_format\": \"s\"\n",
        "        }\n",
        "\n",
        "        output_path = Path(output_dir) / f\"{video_path.stem}.boris\"\n",
        "        with open(output_path, \"w\") as f:\n",
        "            json.dump(boris_project, f, indent=4)\n",
        "        print(f\"BORIS project file for tracked subjects created at: {output_path}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fSota3pkYMvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Video analysis function\n",
        "\n",
        "def analyze_video(video_path: str,\n",
        "                  original_video_path: str,\n",
        "                  model,\n",
        "                  model_classes: List,\n",
        "                  confidence_thresh: float,\n",
        "                  nms_thresh: float, # Kept for signature consistency, but not used in tracking logic\n",
        "                  device: str,\n",
        "                  tracker_config: dict,\n",
        "                  track_thr: float,\n",
        "                  num_tracked_ids: int,\n",
        "                  inference_fps: Optional[int] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Processes a video, performing detection and tracking with performance profiling,\n",
        "    and returns detailed analysis data.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened(): return {}\n",
        "\n",
        "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if video_fps == 0 or total_frames == 0: return {}\n",
        "\n",
        "    tracker = ByteTracker(**tracker_config)\n",
        "    stable_id_map = {sid: None for sid in range(1, num_tracked_ids + 1)}\n",
        "\n",
        "    frame_step = 1\n",
        "    if inference_fps and 0 < inference_fps < video_fps:\n",
        "        frame_step = int(round(video_fps / inference_fps))\n",
        "\n",
        "    all_frame_detections = []\n",
        "    present_timestamps_by_id = {sid: [] for sid in range(1, num_tracked_ids + 1)}\n",
        "\n",
        "    # --- Profiling Timers ---\n",
        "    time_inference, time_tracking, time_postprocess = 0, 0, 0\n",
        "    loop_count = 0\n",
        "\n",
        "    print(f\"Processing video with tracking: {Path(video_path).name}\")\n",
        "    for frame_idx in tqdm(range(0, total_frames, frame_step)):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "\n",
        "        current_time_sec = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0\n",
        "\n",
        "        # --- 1. Time Inference ---\n",
        "        t0 = time.time()\n",
        "        with torch.no_grad(): # Ensure no gradients are computed\n",
        "            mm_results_raw = inference_detector(model, frame)\n",
        "            pred_instances = mm_results_raw.pred_instances\n",
        "            pred_instances = pred_instances[pred_instances.scores > confidence_thresh]\n",
        "        torch.cuda.synchronize() # Wait for GPU to finish\n",
        "        t1 = time.time()\n",
        "        time_inference += (t1 - t0)\n",
        "\n",
        "        # --- 2. Time Tracking ---\n",
        "        t0 = time.time()\n",
        "        det_data_sample = DetDataSample(pred_instances=pred_instances, metainfo=dict(frame_id=frame_idx))\n",
        "        tracked_instances = tracker.track(det_data_sample)\n",
        "        t1 = time.time()\n",
        "        time_tracking += (t1 - t0)\n",
        "\n",
        "        # --- 3. Time Post-processing (Optimized) ---\n",
        "        t0 = time.time()\n",
        "        final_frame_detections = []\n",
        "        if len(tracked_instances) > 0 and 'instances_id' in tracked_instances:\n",
        "\n",
        "            # --- Vectorized Filtering and Sorting on GPU ---\n",
        "            valid_mask = tracked_instances.scores >= track_thr\n",
        "\n",
        "            current_bt_ids = tracked_instances.instances_id[valid_mask]\n",
        "            current_scores = tracked_instances.scores[valid_mask]\n",
        "            current_bboxes = tracked_instances.bboxes[valid_mask]\n",
        "            current_labels = tracked_instances.labels[valid_mask]\n",
        "\n",
        "            # Sort by score on GPU\n",
        "            sort_indices = torch.argsort(current_scores, descending=True)\n",
        "            current_bt_ids = current_bt_ids[sort_indices]\n",
        "            current_scores = current_scores[sort_indices]\n",
        "            current_bboxes = current_bboxes[sort_indices]\n",
        "            current_labels = current_labels[sort_indices]\n",
        "\n",
        "            new_stable_id_map = {sid: None for sid in range(1, num_tracked_ids + 1)}\n",
        "\n",
        "            # Lists to accumulate final tensor data\n",
        "            final_bboxes_list, final_scores_list, final_labels_list, final_stable_ids_list = [], [], [], []\n",
        "\n",
        "            # --- ID Assignment (Hybrid Approach) ---\n",
        "            # Part A: Maintain existing tracks\n",
        "            assigned_bt_ids = set()\n",
        "            for stable_id, prev_bt_id in stable_id_map.items():\n",
        "                if prev_bt_id is not None:\n",
        "                    # Search on the GPU tensor\n",
        "                    match_indices = torch.where(current_bt_ids == prev_bt_id)[0]\n",
        "                    if len(match_indices) > 0:\n",
        "                        idx = match_indices[0]\n",
        "                        new_stable_id_map[stable_id] = prev_bt_id\n",
        "                        assigned_bt_ids.add(prev_bt_id)\n",
        "\n",
        "                        final_bboxes_list.append(current_bboxes[idx])\n",
        "                        final_scores_list.append(current_scores[idx])\n",
        "                        final_labels_list.append(current_labels[idx])\n",
        "                        final_stable_ids_list.append(stable_id)\n",
        "\n",
        "            # Part B: Assign new tracks\n",
        "            if len(assigned_bt_ids) < num_tracked_ids:\n",
        "                for i in range(len(current_bt_ids)):\n",
        "                    if len(assigned_bt_ids) >= num_tracked_ids: break\n",
        "\n",
        "                    bt_id = current_bt_ids[i].item() # .item() is fast for single element\n",
        "                    if bt_id not in assigned_bt_ids:\n",
        "                        for stable_id_slot in range(1, num_tracked_ids + 1):\n",
        "                            if new_stable_id_map[stable_id_slot] is None:\n",
        "                                new_stable_id_map[stable_id_slot] = bt_id\n",
        "                                assigned_bt_ids.add(bt_id)\n",
        "\n",
        "                                final_bboxes_list.append(current_bboxes[i])\n",
        "                                final_scores_list.append(current_scores[i])\n",
        "                                final_labels_list.append(current_labels[i])\n",
        "                                final_stable_ids_list.append(stable_id_slot)\n",
        "                                break\n",
        "            stable_id_map = new_stable_id_map\n",
        "\n",
        "            # --- Final Conversion to CPU ---\n",
        "            if final_bboxes_list:\n",
        "                # Stack all tensors at once\n",
        "                final_bboxes_gpu = torch.stack(final_bboxes_list)\n",
        "                final_scores_gpu = torch.stack(final_scores_list)\n",
        "                final_labels_gpu = torch.stack(final_labels_list)\n",
        "                # Convert Python list of ints to a tensor\n",
        "                final_stable_ids_gpu = torch.tensor(final_stable_ids_list, device=device)\n",
        "\n",
        "                # Move to CPU in one batch\n",
        "                final_bboxes_np = final_bboxes_gpu.cpu().numpy()\n",
        "                final_scores_np = final_scores_gpu.cpu().numpy()\n",
        "                final_labels_np = final_labels_gpu.cpu().numpy()\n",
        "                final_stable_ids_np = final_stable_ids_gpu.cpu().numpy()\n",
        "\n",
        "                # Create final dict list (this loop is unavoidable but now runs on pre-processed CPU data)\n",
        "                for i in range(len(final_bboxes_np)):\n",
        "                    final_frame_detections.append({\n",
        "                        \"bbox\": final_bboxes_np[i].tolist(),\n",
        "                        \"score\": float(final_scores_np[i]),\n",
        "                        \"label\": model_classes[int(final_labels_np[i])],\n",
        "                        \"stable_id\": int(final_stable_ids_np[i])\n",
        "                    })\n",
        "\n",
        "        all_frame_detections.append({\n",
        "            \"frame_index\": frame_idx, \"timestamp_sec\": current_time_sec, \"detections\": final_frame_detections\n",
        "        })\n",
        "\n",
        "        if final_frame_detections:\n",
        "            end_time_sec = current_time_sec + (frame_step / video_fps)\n",
        "            time_block = np.arange(current_time_sec, end_time_sec, 1.0 / video_fps)\n",
        "            for det in final_frame_detections:\n",
        "                sid = det.get(\"stable_id\")\n",
        "                if sid in present_timestamps_by_id:\n",
        "                    present_timestamps_by_id[sid].extend(time_block)\n",
        "\n",
        "        t1 = time.time()\n",
        "        time_postprocess += (t1 - t0)\n",
        "        loop_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # --- Print Performance Breakdown ---\n",
        "    if loop_count > 0:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"--- Performance Breakdown ---\")\n",
        "        print(f\"Frames processed: {loop_count}\")\n",
        "        print(f\"Avg Inference Time:       {time_inference / loop_count * 1000:.2f} ms per frame\")\n",
        "        print(f\"Avg Tracking Time:        {time_tracking / loop_count * 1000:.2f} ms per frame\")\n",
        "        print(f\"Avg Post-processing Time: {time_postprocess / loop_count * 1000:.2f} ms per frame\")\n",
        "        print(f\"Total Avg Time per Frame: {(time_inference + time_tracking + time_postprocess) / loop_count * 1000:.2f} ms\")\n",
        "        print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    for sid in present_timestamps_by_id:\n",
        "        present_timestamps_by_id[sid] = sorted(list(set(present_timestamps_by_id[sid])))\n",
        "\n",
        "    return {\n",
        "        \"video_filepath\": original_video_path,\n",
        "        \"processed_video_path\": video_path,\n",
        "        \"fps\": video_fps,\n",
        "        \"total_frames\": total_frames,\n",
        "        \"detailed_detections\": all_frame_detections,\n",
        "        \"timestamps_summary_by_id\": present_timestamps_by_id\n",
        "    }"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3k4htzi2Blu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper class for visibility timeline visualization.\n",
        "\n",
        "class VideoAnalysisVisualizer:\n",
        "    def __init__(self, original_video_path: str, analysis_data: dict, output_dir: str, dpi: int = 300, thickness: int = 2):\n",
        "        self.original_video_path = original_video_path\n",
        "        self.analysis_data = analysis_data\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.dpi = dpi\n",
        "        self.thickness = thickness\n",
        "        self.video_stem = Path(original_video_path).stem\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.clip = None\n",
        "        try:\n",
        "            # Always open the original, high-quality video for stills\n",
        "            self.clip = VideoFileClip(original_video_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error opening video {original_video_path} with MoviePy: {e}. Visualizations will not be generated.\")\n",
        "\n",
        "    def _get_video_dimensions(self, video_path: str) -> Tuple[int, int]:\n",
        "        \"\"\"Helper to get (width, height) of a video.\"\"\"\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            if not cap.isOpened(): return (0, 0)\n",
        "            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "            cap.release()\n",
        "            return (width, height)\n",
        "        except Exception:\n",
        "            return (0, 0)\n",
        "\n",
        "    def plot_timeline(\n",
        "        self,\n",
        "        target_still_timestamp_sec: Optional[float] = None,\n",
        "        fig_filename_prefix: str = \"timeline\",\n",
        "        raster_cmap: str = \"viridis\"\n",
        "    ):\n",
        "        if not self.clip: return\n",
        "        detailed_detections = self.analysis_data.get(\"detailed_detections\")\n",
        "        if not detailed_detections:\n",
        "            print(f\"No detection data available for {self.original_video_path}. Skipping timeline.\")\n",
        "            return\n",
        "\n",
        "        # --- Bounding Box Rescaling Logic ---\n",
        "        original_dims = self._get_video_dimensions(self.original_video_path)\n",
        "        processed_path = self.analysis_data.get(\"processed_video_path\", self.original_video_path)\n",
        "        processed_dims = self._get_video_dimensions(processed_path)\n",
        "\n",
        "        scale_x, scale_y = 1.0, 1.0\n",
        "        if all(d > 0 for d in original_dims) and all(d > 0 for d in processed_dims) and original_dims != processed_dims:\n",
        "            scale_x = original_dims[0] / processed_dims[0]\n",
        "            scale_y = original_dims[1] / processed_dims[1]\n",
        "            print(f\"Rescaling bboxes for visualization. X-Factor: {scale_x:.2f}, Y-Factor: {scale_y:.2f}\")\n",
        "\n",
        "        # Find all unique stable IDs to create the raster plot rows\n",
        "        all_sids = sorted(list(set(det['stable_id'] for fd in detailed_detections for det in fd.get('detections', []) if 'stable_id' in det)))\n",
        "\n",
        "        if not all_sids:\n",
        "            raster_row_labels = [\"Detection\"]\n",
        "            visibility_matrix = np.zeros((1, len(detailed_detections)))\n",
        "            for frame_idx, frame_data in enumerate(detailed_detections):\n",
        "                if frame_data.get(\"detections\"):\n",
        "                    visibility_matrix[0, frame_idx] = 1\n",
        "        else:\n",
        "            raster_row_labels = [f\"ID {sid}\" for sid in all_sids]\n",
        "            sid_to_row_idx = {sid: i for i, sid in enumerate(all_sids)}\n",
        "            visibility_matrix = np.zeros((len(all_sids), len(detailed_detections)))\n",
        "            for frame_idx, frame_data in enumerate(detailed_detections):\n",
        "                for det in frame_data.get(\"detections\", []):\n",
        "                    sid = det.get(\"stable_id\")\n",
        "                    if sid in sid_to_row_idx:\n",
        "                        visibility_matrix[sid_to_row_idx[sid], frame_idx] = 1\n",
        "\n",
        "        # --- Choose Still Frame ---\n",
        "        frames_with_detections = [fd for fd in detailed_detections if fd.get(\"detections\")]\n",
        "        if not frames_with_detections:\n",
        "            print(f\"No frames with any detections found. Skipping timeline figure.\")\n",
        "            return\n",
        "\n",
        "        chosen_frame_data = random.choice(frames_with_detections)\n",
        "        if target_still_timestamp_sec is not None:\n",
        "            chosen_frame_data = min(frames_with_detections, key=lambda fd: abs(fd[\"timestamp_sec\"] - target_still_timestamp_sec))\n",
        "\n",
        "        still_timestamp = chosen_frame_data[\"timestamp_sec\"]\n",
        "        still_detections = chosen_frame_data.get(\"detections\", [])\n",
        "\n",
        "        try:\n",
        "            still_frame_rgb = self.clip.get_frame(still_timestamp)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting still frame: {e}. Skipping timeline.\")\n",
        "            return\n",
        "\n",
        "        timestamps_axis = np.array([fd[\"timestamp_sec\"] for fd in detailed_detections])\n",
        "\n",
        "        # --- Apply scaling to the detections for the still frame ---\n",
        "        rescaled_detections = []\n",
        "        for det in still_detections:\n",
        "            rescaled_det = det.copy()\n",
        "            x1, y1, x2, y2 = rescaled_det[\"bbox\"]\n",
        "            rescaled_det[\"bbox\"] = [x1 * scale_x, y1 * scale_y, x2 * scale_x, y2 * scale_y]\n",
        "            rescaled_detections.append(rescaled_det)\n",
        "\n",
        "        # --- Plotting ---\n",
        "        still_bgr_with_boxes = visualize_frame_with_detections(\n",
        "            cv2.cvtColor(still_frame_rgb, cv2.COLOR_RGB2BGR),\n",
        "            rescaled_detections,\n",
        "            thickness=self.thickness\n",
        "        )\n",
        "        still_rgb_with_boxes = cv2.cvtColor(still_bgr_with_boxes, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        fig_height_ratio = len(raster_row_labels) or 1\n",
        "        fig = plt.figure(figsize=(10, 4 + fig_height_ratio * 0.5))\n",
        "        gs = fig.add_gridspec(2, 1, height_ratios=[3, fig_height_ratio], hspace=0.15)\n",
        "\n",
        "        ax0 = fig.add_subplot(gs[0])\n",
        "        ax0.imshow(still_rgb_with_boxes)\n",
        "        ax0.axis(\"off\")\n",
        "        ax0.set_title(f\"Frame at {still_timestamp:.2f}s\", fontsize=10)\n",
        "\n",
        "        ax1 = fig.add_subplot(gs[1])\n",
        "        ax1.imshow(visibility_matrix, aspect=\"auto\", cmap=raster_cmap, interpolation='nearest', extent=[timestamps_axis[0], timestamps_axis[-1], -0.5, len(raster_row_labels)-0.5])\n",
        "        ax1.set_yticks(range(len(raster_row_labels)))\n",
        "        ax1.set_yticklabels(raster_row_labels)\n",
        "        ax1.set_xlabel(\"Time (s)\")\n",
        "        ax1.set_ylabel(\"Tracked ID\" if all_sids else \"Detections\")\n",
        "        ax1.axvline(x=still_timestamp, color='red', linestyle='--', linewidth=1.5)\n",
        "\n",
        "        plt.tight_layout(pad=0.5)\n",
        "        fig_path = self.output_dir / f\"{self.video_stem}_ID_timeline.png\"\n",
        "        pdf_fig_path = self.output_dir / f\"{self.video_stem}_ID_timeline.pdf\"\n",
        "\n",
        "        try:\n",
        "            plt.savefig(fig_path, dpi=self.dpi, bbox_inches='tight')\n",
        "            plt.savefig(pdf_fig_path, dpi=self.dpi, bbox_inches='tight', format='pdf')\n",
        "            print(f\"ID timeline figure saved to: {fig_path} and {pdf_fig_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving timeline figure: {e}\")\n",
        "\n",
        "        plt.show()\n",
        "        plt.close(fig)\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Closes the video file clip.\"\"\"\n",
        "        if self.clip:\n",
        "            self.clip.close()\n",
        "            self.clip = None\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "w3_unDAn2L1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Load Your Video Data**\n",
        "\n",
        "Now it's time to choose the video(s) you want to analyze. You have three easy options.\n",
        "\n",
        "**Please choose one option in the form cell below:**\n",
        "\n",
        "*   **Option 1: Use the Demo Video (Default)**\n",
        "    *   This is the simplest way to get started. We'll automatically download a sample video of ring-tailed lemurs for you. Just run the next cell without changing anything.\n",
        "\n",
        "*   **Option 2: Use Your Own Video via a Google Drive Link**\n",
        "    *   If you have a single video on Google Drive, this is a great option.\n",
        "    *   **How-to**: In Google Drive, right-click your video, select **Share > Share**, and set \"General access\" to **\"Anyone with the link\"**. Copy the link and paste it into the `video_gdrive_link` field below.\n",
        "\n",
        "*   **Option 3: Use a Folder from Your Mounted Google Drive**\n",
        "    *   This is the best option for analyzing multiple videos at once.\n",
        "    *   **How-to**: First, check the `mount_my_drive` box in the form. You'll be asked to authorize Colab to access your Google Drive. Then, update the `mounted_drive_path` to point to the folder containing your videos (e.g., `/content/drive/MyDrive/MyPrimateVideos`).\n",
        "\n",
        "**After selecting your option, run the cell to load the data.**"
      ],
      "metadata": {
        "id": "gsGZOKbA5sjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.1 Configure Video Source\n",
        "#@markdown 1. Choose your data source from the dropdown menu.\n",
        "#@markdown 2. Fill in the corresponding fields below if needed.\n",
        "#@markdown 3. Run the cell.\n",
        "\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "# --- Form Parameters ---\n",
        "data_source_option = \"Use a Path from Mounted Google Drive\"  #@param [\"Use Demo Video\", \"Use a Google Drive Link\", \"Use a Path from Mounted Google Drive\"]\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### For Option 2: Provide a Google Drive Link\n",
        "#@markdown *If using this option, paste your shareable link here.*\n",
        "video_gdrive_link = \"PASTE_YOUR_GOOGLE_DRIVE_LINK_HERE\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### For Option 3: Provide a Path from Your Mounted Drive\n",
        "#@markdown *If using this option, set the path to your video file or folder.*\n",
        "mounted_drive_path = \"/content/drive/MyDrive/PrimateFace/demo-input-videos\"  #@param {type:\"string\"}\n",
        "#@markdown *Check this box to mount your Google Drive (required for Option 3).*\n",
        "mount_my_drive = True  #@param {type:\"boolean\"}\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "# Initialize a global path variable\n",
        "VIDEO_INPUT_PATH = \"\"\n",
        "\n",
        "# --- Logic to handle the user's choice ---\n",
        "try:\n",
        "    if data_source_option == \"Use Demo Video\":\n",
        "        print(\"▶️ Option 1: Using the demo video.\")\n",
        "        demo_video_link = \"https://drive.google.com/file/d/1VHLn_zdkwaYrtgjgl_Fz_jTCYVGIu5Il/view?usp=drive_link\"\n",
        "        demo_video_filename = \"demo_cayo_mac.mp4\"\n",
        "        print(f\"Downloading demo video to '{demo_video_filename}'...\")\n",
        "        gdown.download(demo_video_link, demo_video_filename, quiet=False, fuzzy=True)\n",
        "        if not os.path.exists(demo_video_filename):\n",
        "            raise FileNotFoundError(\"Demo video download failed.\")\n",
        "        VIDEO_INPUT_PATH = demo_video_filename\n",
        "\n",
        "    elif data_source_option == \"Use a Google Drive Link\":\n",
        "        print(\"▶️ Option 2: Using a Google Drive link.\")\n",
        "        if \"PASTE_YOUR\" in video_gdrive_link:\n",
        "            raise ValueError(\"Please paste your Google Drive link into the 'video_gdrive_link' field.\")\n",
        "\n",
        "        linked_video_filename = \"user_gdrive_video.mp4\"\n",
        "        print(f\"Downloading video from link to '{linked_video_filename}'...\")\n",
        "        gdown.download(video_gdrive_link, linked_video_filename, quiet=False, fuzzy=True)\n",
        "        if not os.path.exists(linked_video_filename):\n",
        "            raise FileNotFoundError(\"Video download from your link failed. Please check the URL and sharing permissions.\")\n",
        "        VIDEO_INPUT_PATH = linked_video_filename\n",
        "\n",
        "    elif data_source_option == \"Use a Path from Mounted Google Drive\":\n",
        "        print(\"▶️ Option 3: Using a path from your mounted Google Drive.\")\n",
        "        if mount_my_drive:\n",
        "            print(\"Mounting Google Drive...\")\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"Drive mounted successfully.\")\n",
        "        else:\n",
        "            print(\"Assuming Google Drive is already mounted. If you see an error, check the 'mount_my_drive' box.\")\n",
        "\n",
        "        if not Path(mounted_drive_path).exists():\n",
        "            raise FileNotFoundError(f\"The path '{mounted_drive_path}' does not exist in your Google Drive. Please check the path.\")\n",
        "        VIDEO_INPUT_PATH = mounted_drive_path\n",
        "\n",
        "    # --- Final Validation ---\n",
        "    video_files = get_video_paths(VIDEO_INPUT_PATH)\n",
        "    print(\"\\n✅ Success! The following video(s) are ready for analysis:\")\n",
        "    for vf in video_files:\n",
        "        print(f\" - {Path(vf).name}\")\n",
        "\n",
        "except (ValueError, FileNotFoundError) as e:\n",
        "    print(f\"\\n❌ Error: {e}\")\n",
        "    print(\"Please correct the settings in the form and run the cell again.\")\n",
        "    video_files = [] # Ensure it's empty on failure"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ww4iDR0FUrEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Configure analysis & pre-process videos**\n",
        "\n"
      ],
      "metadata": {
        "id": "F-BtVqfLEmMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [Optional] 4.1 Pre-process Videos for Faster Analysis\n",
        "#@markdown This step handles the one-time video downsampling.\n",
        "#@markdown Set a lower value (e.g., 360) to speed up analysis, or **set to 0 to disable.**\n",
        "#@markdown Note that downsampling may result in less accurate face detections.\n",
        "\n",
        "target_processing_height = 360 #@param {type:\"integer\"}\n",
        "\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "analysis_jobs = []\n",
        "\n",
        "if 'video_files' in locals() and video_files:\n",
        "    print(\"--- Pre-processing Videos ---\")\n",
        "    with tqdm(total=len(video_files), desc=\"Processing Videos\") as pbar:\n",
        "        for original_path in video_files:\n",
        "            job = {\n",
        "                'original_path': original_path,\n",
        "                'processing_path': original_path, # Default to original\n",
        "                'temp_file_to_delete': None\n",
        "            }\n",
        "            if target_processing_height > 0:\n",
        "                temp_path = create_downsampled_video(original_path, target_processing_height)\n",
        "                if temp_path:\n",
        "                    job['processing_path'] = temp_path\n",
        "                    job['temp_file_to_delete'] = temp_path\n",
        "\n",
        "            analysis_jobs.append(job)\n",
        "            pbar.set_description(f\"Processed '{Path(original_path).name}'\")\n",
        "            pbar.update(1)\n",
        "\n",
        "    print(\"\\n✅ Pre-processing complete. Ready for interactive threshold selection.\")\n",
        "else:\n",
        "    print(\"❌ No video files loaded. Please run the 'Configure Video Source' cell successfully.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AjYmokw0Xbab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Calibrate the Detection Confidence**\n",
        "\n",
        "This tool helps you choose the best confidence threshold for your videos. A **high threshold** is strict (fewer, but more accurate detections), while a **low threshold** is lenient (more detections, but potentially more errors).\n",
        "\n",
        "**Your Goal:** Move the slider to find the \"sweet spot\" where green boxes correctly identify most faces without picking up too much background noise."
      ],
      "metadata": {
        "id": "oZwtwOm5YZzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5.1 Run me to explore confidence thresholds!\n",
        "num_imgs_to_display = 7 #@param {type: \"integer\"}\n",
        "bbox_thickness = 4 #@param {type: \"integer\"}\n",
        "\n",
        "# This global variable will store the slider's value for the next step.\n",
        "final_confidence_threshold = 0.4 # Default value\n",
        "def interactive_threshold_explorer(\n",
        "    jobs_to_sample: List[Dict],\n",
        "    model: Any,\n",
        "    model_classes: List[str],\n",
        "    nms_thresh: float,\n",
        "    device: str,\n",
        "    num_imgs: int,\n",
        "    bbox_thick: int\n",
        "):\n",
        "    print(f\"Selecting up to {num_imgs} random frames for interactive testing...\")\n",
        "\n",
        "    sample_data = []\n",
        "    samples_per_video = int(np.ceil(num_imgs / len(jobs_to_sample)))\n",
        "\n",
        "    with tqdm(total=len(jobs_to_sample), desc=\"Sampling frames\") as pbar:\n",
        "        for job in jobs_to_sample:\n",
        "            # Use the pre-processed video path for sampling\n",
        "            processing_path = job['processing_path']\n",
        "\n",
        "            # --- MODIFIED: Use cv2.VideoCapture for more robust video reading ---\n",
        "            cap = cv2.VideoCapture(processing_path)\n",
        "            if not cap.isOpened():\n",
        "                print(f\"Warning: Could not open {processing_path} with OpenCV, skipping.\")\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            duration = total_frames / fps if fps > 0 else 0\n",
        "\n",
        "            if duration <= 1.0:\n",
        "                pbar.update(1)\n",
        "                cap.release()\n",
        "                continue\n",
        "            # --- END MODIFIED SECTION ---\n",
        "\n",
        "            possible_ts = np.arange(0.5, duration - 0.5, 0.5)\n",
        "            timestamps_to_sample = random.sample(list(possible_ts), k=min(samples_per_video, len(possible_ts)))\n",
        "\n",
        "            for t in timestamps_to_sample:\n",
        "                if len(sample_data) >= num_imgs: break\n",
        "\n",
        "                # --- MODIFIED: Read frame from cv2.VideoCapture object ---\n",
        "                frame_index_to_get = int(t * fps)\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index_to_get)\n",
        "                ret, frame_bgr = cap.read()\n",
        "                if not ret: continue\n",
        "                # --- END MODIFIED SECTION ---\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    raw_preds = inference_detector(model, frame_bgr)\n",
        "\n",
        "                sample_data.append({\n",
        "                    \"display_frame_bgr\": frame_bgr, # Pass BGR directly\n",
        "                    \"raw_preds\": raw_preds,\n",
        "                    \"source_video\": Path(job['original_path']).name,\n",
        "                    \"timestamp\": t\n",
        "                })\n",
        "\n",
        "            pbar.update(1)\n",
        "            cap.release() # --- MODIFIED: Release the capture object\n",
        "            if len(sample_data) >= num_imgs: break\n",
        "\n",
        "    if not sample_data:\n",
        "        print(\"Error: Could not extract any sample frames. Please check that your video files are not corrupt and are in a common format (e.g., MP4).\")\n",
        "        return\n",
        "\n",
        "    # This is the function that the slider will call\n",
        "    def update_plot(confidence_thr):\n",
        "        global final_confidence_threshold\n",
        "        final_confidence_threshold = confidence_thr # Update global variable\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        cols = int(np.ceil(np.sqrt(len(sample_data))))\n",
        "        rows = int(np.ceil(len(sample_data) / cols))\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(cols * 4.5, rows * 4.5))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for i, sample in enumerate(sample_data):\n",
        "            filtered_dets = filter_and_nms_raw_detections(sample[\"raw_preds\"], model_classes, confidence_thr, nms_thresh, device)\n",
        "\n",
        "            # --- MODIFIED: Use BGR frame directly for visualization ---\n",
        "            frame_with_boxes = visualize_frame_with_detections(sample[\"display_frame_bgr\"], filtered_dets, thickness=bbox_thick)\n",
        "\n",
        "            avg_conf_text = \"\"\n",
        "            if filtered_dets:\n",
        "                avg_conf = np.mean([d['score'] for d in filtered_dets])\n",
        "                avg_conf_text = f\" | Avg Conf: {avg_conf:.2f}\"\n",
        "\n",
        "            title_text = f\"{sample['source_video']}\\n@{sample['timestamp']:.1f}s | {len(filtered_dets)} dets{avg_conf_text}\"\n",
        "\n",
        "            axes[i].imshow(cv2.cvtColor(frame_with_boxes, cv2.COLOR_BGR2RGB)) # Convert to RGB only for plotting\n",
        "            # --- END MODIFIED SECTION ---\n",
        "\n",
        "            axes[i].set_title(title_text, fontsize=9)\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        for j in range(len(sample_data), len(axes)):\n",
        "            axes[j].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Create and display the interactive widget\n",
        "    interact(\n",
        "        update_plot,\n",
        "        confidence_thr=FloatSlider(min=0.05, max=0.99, step=0.01, value=0.4, description='Confidence Thr.', continuous_update=False, readout_format='.2f', layout={'width': '500px'})\n",
        "    )\n",
        "\n",
        "# --- Main execution logic for this cell ---\n",
        "if 'analysis_jobs' in locals() and analysis_jobs:\n",
        "    # Load the model if it hasn't been loaded yet\n",
        "    if 'mmdet_model' not in globals() or mmdet_model is None:\n",
        "        mmdet_model = load_mmdetection_model(MMDET_CONFIG_PATH, MMDET_CHECKPOINT_PATH, MMDET_DEVICE)\n",
        "\n",
        "    model_class_names = get_model_class_names(mmdet_model)\n",
        "    print(f\"Model classes found: {model_class_names}\")\n",
        "\n",
        "    interactive_threshold_explorer(\n",
        "        jobs_to_sample=analysis_jobs,\n",
        "        model=mmdet_model,\n",
        "        model_classes=model_class_names,\n",
        "        nms_thresh=DEFAULT_NMS_THRESHOLD,\n",
        "        device=MMDET_DEVICE,\n",
        "        num_imgs=num_imgs_to_display,\n",
        "        bbox_thick=bbox_thickness\n",
        "    )\n",
        "else:\n",
        "    print(\"❌ Analysis jobs not created. Please run the 'Configure Analysis' cell first.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QYUgrPnPZzh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Run Full Analysis**\n",
        "**Action Required**\n",
        "\n",
        "This final cell runs the complete analysis on all your videos using the parameters you set below. It will save a detailed JSON file and the `.boris` project file(s) to the output directory.\n",
        "\n",
        "**Set your desired parameters using the interactive form below.**\n",
        "*   **`final_confidence_threshold`**: The value you determined in the interactive step.\n",
        "*   **`use_fast_inference`**: Check this box to speed up the analysis.\n",
        "*   **`inference_fps`**: If using fast inference, this is how many frames per second the model will analyze. A lower number is faster."
      ],
      "metadata": {
        "id": "8UBj8vZk_Ce0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6.1 Run video inference\n",
        "#@markdown This cell uses the confidence threshold from the previous step and the settings below to run the final analysis.\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Analysis & Output Configuration\n",
        "#@markdown Set this to the value you chose using the interactive slider.\n",
        "final_confidence_threshold = 0.75 #@param {type:\"slider\", min:0.1, max:0.99, step:0.01}\n",
        "\n",
        "#@markdown Select the analysis speed. \"Fast\" is less accurate but much faster.\n",
        "analysis_mode = \"Fast Inference (skips frames)\" #@param [\"Full Video (accurate, slow)\", \"Fast Inference (skips frames)\"]\n",
        "\n",
        "#@markdown **Note:** The FPS slider is only used if you select \"Fast Inference\".\n",
        "inference_fps = 5 #@param {type:\"slider\", min:1, max:30, step:1}\n",
        "\n",
        "#@markdown Set the directory and filename for your results.\n",
        "OUTPUT_DIR = \"analysis_results\" #@param {type:\"string\"}\n",
        "OUTPUT_JSON_FILENAME = \"video_face_analysis_results.json\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Select which output files you want to generate.\n",
        "generate_json_file = True #@param {type:\"boolean\"}\n",
        "generate_boris_file = True #@param {type:\"boolean\"}\n",
        "generate_timeline_figure = True #@param {type:\"boolean\"}\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "# --- Setup from Parameters ---\n",
        "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "output_json_path = Path(OUTPUT_DIR) / OUTPUT_JSON_FILENAME\n",
        "\n",
        "if \"Fast Inference\" in analysis_mode:\n",
        "    inference_fps_to_use = inference_fps\n",
        "else:\n",
        "    inference_fps_to_use = None\n",
        "\n",
        "# Create the master results dictionary, storing the chosen parameters for reproducibility\n",
        "master_results = {\n",
        "    \"parameters\": {\n",
        "        \"confidence_threshold\": final_confidence_threshold,\n",
        "        \"nms_threshold\": DEFAULT_NMS_THRESHOLD,\n",
        "        \"track_confidence_threshold\": DEFAULT_TRACK_CONF_THRESHOLD,\n",
        "        \"num_tracked_ids\": DEFAULT_NUM_TRACKED_IDS,\n",
        "        \"inference_fps\": inference_fps_to_use,\n",
        "        \"target_processing_height\": target_processing_height\n",
        "    },\n",
        "    \"videos\": []\n",
        "}\n",
        "\n",
        "# --- Main Processing Loop ---\n",
        "if 'analysis_jobs' in locals() and analysis_jobs:\n",
        "    if 'mmdet_model' not in globals() or mmdet_model is None:\n",
        "        print(\"❌ Error: MMDetection model is not loaded. Please run the model loading cell first.\")\n",
        "    else:\n",
        "        for job in analysis_jobs:\n",
        "            original_video_path = job['original_path']\n",
        "            processing_video_path = job['processing_path']\n",
        "            temp_file_to_delete = job['temp_file_to_delete']\n",
        "\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(f\"Analyzing Video: {Path(original_video_path).name}\")\n",
        "            if temp_file_to_delete:\n",
        "                print(f\"--> Using temporary downsampled version: {Path(processing_video_path).name}\")\n",
        "\n",
        "            video_analysis_data = analyze_video(\n",
        "                video_path=processing_video_path,\n",
        "                original_video_path=original_video_path,\n",
        "                model=mmdet_model,\n",
        "                model_classes=model_class_names,\n",
        "                confidence_thresh=final_confidence_threshold,\n",
        "                nms_thresh=DEFAULT_NMS_THRESHOLD,\n",
        "                device=MMDET_DEVICE,\n",
        "                inference_fps=inference_fps_to_use,\n",
        "                tracker_config=DEFAULT_TRACKER_CONFIG,\n",
        "                track_thr=DEFAULT_TRACK_CONF_THRESHOLD,\n",
        "                num_tracked_ids=DEFAULT_NUM_TRACKED_IDS\n",
        "            )\n",
        "\n",
        "            if video_analysis_data:\n",
        "                master_results[\"videos\"].append(video_analysis_data)\n",
        "                if generate_timeline_figure:\n",
        "                    print(f\"\\nGenerating timeline figure for {Path(original_video_path).name}...\")\n",
        "                    visualizer = VideoAnalysisVisualizer(\n",
        "                        original_video_path,\n",
        "                        video_analysis_data,\n",
        "                        OUTPUT_DIR,\n",
        "                        thickness=bbox_thickness\n",
        "                    )\n",
        "                    visualizer.plot_timeline()\n",
        "                    visualizer.close()\n",
        "            else:\n",
        "                print(f\"Analysis failed or produced no data for {original_video_path}\")\n",
        "\n",
        "            # Clean up the temporary file for this job after it's processed\n",
        "            if temp_file_to_delete:\n",
        "                print(f\"Cleaning up temporary file: {temp_file_to_delete}\")\n",
        "                try:\n",
        "                    os.unlink(temp_file_to_delete)\n",
        "                except OSError as e:\n",
        "                    print(f\"  Error removing temp file: {e}\")\n",
        "\n",
        "        # --- Final Save Data Outputs ---\n",
        "        if master_results[\"videos\"]:\n",
        "            print(\"\\n--- Saving Output Files ---\")\n",
        "            if generate_json_file:\n",
        "                save_results_to_json(master_results, str(output_json_path))\n",
        "            if generate_boris_file:\n",
        "                save_boris_project(master_results, OUTPUT_DIR)\n",
        "        else:\n",
        "            print(\"No videos were successfully processed. No output files saved.\")\n",
        "else:\n",
        "    print(\"❌ No analysis jobs found. Please run the data loading and pre-processing cells first.\")\n",
        "\n",
        "print(\"\\n--- ✅ Processing Finished ---\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_1lpx4L6ahVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Preview the Output Files**\n",
        "\n",
        "Let's take a look at the files we just created in the `analysis_results` directory. This will give you a clear idea of what each file contains."
      ],
      "metadata": {
        "id": "_oVMJRS-3iAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display files\n",
        "\n",
        "import json\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "from rich.syntax import Syntax\n",
        "from rich.panel import Panel\n",
        "\n",
        "# Create a console object for printing\n",
        "console = Console()\n",
        "\n",
        "# --- Preview the Detailed Analysis JSON file ---\n",
        "if generate_json_file and 'output_json_path' in locals() and output_json_path.exists():\n",
        "\n",
        "    panel_title = f\"Preview of Detailed Analysis File: [bold cyan]{output_json_path.name}[/bold cyan]\"\n",
        "\n",
        "    with open(output_json_path, 'r') as f:\n",
        "        analysis_data = json.load(f)\n",
        "\n",
        "    # --- Parameters Table ---\n",
        "    param_table = Table(title=\"Analysis Parameters\", show_header=True, header_style=\"bold magenta\")\n",
        "    param_table.add_column(\"Parameter\", style=\"dim\")\n",
        "    param_table.add_column(\"Value\")\n",
        "\n",
        "    for key, val in analysis_data.get(\"parameters\", {}).items():\n",
        "        param_table.add_row(key, str(val))\n",
        "\n",
        "    # --- Example Detection Panel ---\n",
        "    first_video = analysis_data[\"videos\"][0] if analysis_data.get(\"videos\") else None\n",
        "    first_detection_json = \"{}\"\n",
        "    if first_video:\n",
        "        first_detection = next((d for d in first_video.get('detailed_detections', []) if d['detections']), None)\n",
        "        if first_detection:\n",
        "            first_detection_json = json.dumps(first_detection, indent=2)\n",
        "\n",
        "    json_preview = Syntax(first_detection_json, \"json\", theme=\"monokai\", line_numbers=True)\n",
        "\n",
        "    # Render the analysis preview in a panel\n",
        "    console.print(Panel.fit(param_table, title=panel_title, border_style=\"green\"))\n",
        "    console.print(Panel(json_preview, title=\"[bold green]Example of First Detected Frame Data[/bold green]\", border_style=\"green\"))\n",
        "\n",
        "\n",
        "# --- Preview the BORIS Project file ---\n",
        "if generate_boris_file and 'master_results' in locals() and master_results.get(\"videos\"):\n",
        "    first_video_path = Path(master_results[\"videos\"][0][\"video_filepath\"])\n",
        "    boris_file_path = Path(OUTPUT_DIR) / f\"{first_video_path.stem}.boris\"\n",
        "\n",
        "    if boris_file_path.exists():\n",
        "        panel_title = f\"Preview of BORIS Project File: [bold cyan]{boris_file_path.name}[/bold cyan]\"\n",
        "\n",
        "        with open(boris_file_path, 'r') as f:\n",
        "            boris_data = json.load(f)\n",
        "\n",
        "        obs_key = list(boris_data.get('observations', {}).keys())[0]\n",
        "        events = boris_data['observations'][obs_key].get('events', [])\n",
        "\n",
        "        # --- Events Table ---\n",
        "        events_table = Table(title=f\"First 10 'face present' Events\", show_header=True, header_style=\"bold magenta\")\n",
        "        events_table.add_column(\"Timestamp\", justify=\"right\")\n",
        "        events_table.add_column(\"Behavior\")\n",
        "        events_table.add_column(\"Event Type\", style=\"dim\")\n",
        "\n",
        "        if events:\n",
        "            for event in events[:10]:\n",
        "                events_table.add_row(f\"{event[0]:.2f}s\", event[2], event[5])\n",
        "\n",
        "        # Render the BORIS preview in a panel\n",
        "        console.print(Panel.fit(events_table, title=panel_title, border_style=\"blue\"))\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZlFwQTb-3hST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Summary and Next Steps**\n",
        "\n",
        "Congratulations! You have successfully completed the automated video timestamping process.\n",
        "\n",
        "**Here's a recap of what we accomplished:**\n",
        "1.  We loaded a pre-trained `PrimateFace` detection model.\n",
        "2.  We used an interactive widget to visually determine the best detection **confidence threshold** for your specific video(s).\n",
        "3.  We ran a full analysis using your chosen threshold to identify every frame containing a face.\n",
        "4.  We generated two key output files, which are now saved in the `analysis_results` directory:\n",
        "    *   `video_face_analysis_results.json`: A detailed data file containing the bounding box, score, and timestamp for every single detection.\n",
        "    *   `[your_video_name].boris`: A BORIS project file, ready for manual annotation.\n",
        "\n",
        "**Your next step is to open the generated `.boris` file in the BORIS application.** You will find that the timeline already contains a \"face present\" state event, accurately marking all the periods where a face was visible. You may need to change the location of the video file(s), typically with Observation > Edit observation.\n",
        "\n",
        "You can now use this as a foundation to add your own, more detailed behavioral codes, saving valuable annotation time."
      ],
      "metadata": {
        "id": "kSoz7yLTbDmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Resources\n",
        "1. [PrimateFace](https://github.com/PrimateFace/PrimateFace)\n",
        "2. [mmdetection](https://github.com/open-mmlab/mmdetection)\n",
        "3. [roboflow](https://roboflow.com/)"
      ],
      "metadata": {
        "id": "S4hQAT4-3uuP"
      }
    }
  ]
}