{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# PrimateFace Tutorial: Gaze-Following Heuristic\n",
        "\n",
        "| GitHub Repo | Paper | Project Page |\n",
        "|---|---|---|\n",
        "| [PrimateFace](https://github.com/PrimateFace/PrimateFace) | [PrimateFace](https://arxiv.org/abs/10000) | [PrimateFace](https://primateface.github.io/) |\n",
        "\n",
        "Welcome! This tutorial notebook demonstrates a gaze-following heuristic for videos containing two primates. It uses models from the **PrimateFace** and **Gazelle** projects to analyze social attention dynamics.\n",
        "\n",
        "### This notebook will:\n",
        "1.  **Set up the environment** with all necessary deep learning libraries (mmdetection, mmpose, gazelle).\n",
        "2.  **Download pre-trained models** for face detection and gaze estimation.\n",
        "3.  **Process a two-primate video**: It detects and tracks both individuals, then estimates the gaze direction for one primate (the \"gazer\").\n",
        "4.  **Engineer Features**: It calculates features like the gazer's head yaw, gaze uncertainty (represented as a cone), and the relative position of the second primate (the \"follower\").\n",
        "5.  **Train & Evaluate a Gaze-Following Model**: It trains several classifiers (e.g., Logistic Regression, SVM) to predict if the follower will turn its head toward the gazer's direction of attention within a short time window.\n",
        "6.  **Visualize Results**: It generates several plots, including Precision-Recall curves for the classifiers and qualitative examples of the gaze analysis.\n",
        "\n",
        "---\n",
        "### **Quick Start Instructions**\n",
        "\n",
        "*   **Set Your Runtime to GPU**: Go to **Runtime > Change runtime type > T4 GPU**. This is essential for performance.\n",
        "*   **Run Cells Sequentially**: Click the \"Play\" button on each cell to run it. The first few cells handle setup and may take a few moments.\n",
        "*   **Restarting**: Installing the dependencies requires restarting the Runtime. You can do this by clicking the **Runtime** menu and selecting **Restart session**. Follow the on-screen prompts.\n",
        "\n",
        "This notebook provides a complete pipeline from video to analysis. Let's begin!"
      ],
      "metadata": {
        "id": "K8UXtpNAcYJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Set up the environment (this will take a couple of min)**\n"
      ],
      "metadata": {
        "id": "3tPjRKDYcaoo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vIxBNbZMZzUz"
      },
      "outputs": [],
      "source": [
        "#@title 1.1 Check GPU availability\n",
        "# If this command fails, go to Runtime > Change runtime type and select \"T4 GPU\".\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.2 Install Core Dependencies\n",
        "%%capture\n",
        "# Uninstall conflicting packages first to ensure compatibility\n",
        "!pip uninstall -y fastai spacy thinc pymc pytensor jax jaxlib yfinance\n",
        "\n",
        "# Install the scientific computing stack and video processing libraries\n",
        "!pip install --no-cache-dir \"numpy<1.24\" \"pandas<2.0\" \"opencv-python-headless<4.9\" \"scikit-learn<1.4\" \\\n",
        "               \"matplotlib<3.8\" \"scipy<1.12\" moviepy==1.0.3 imageio imageio-ffmpeg tqdm filterpy\n",
        "\n",
        "# Install PyTorch for the correct CUDA version\n",
        "!pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hVu7xH-HceHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.3 Install MM and Gazelle Libraries\n",
        "%%capture\n",
        "# Install open-mmlab libraries\n",
        "%pip install -U openmim\n",
        "!mim install \"mmengine==0.10.3\"\n",
        "!mim install \"mmcv==2.1.0\"\n",
        "\n",
        "# Install mmdetection\n",
        "!rm -rf mmdetection\n",
        "!git clone https://github.com/open-mmlab/mmdetection.git\n",
        "%cd mmdetection\n",
        "%pip install -e .\n",
        "\n",
        "# Install mmpose\n",
        "%pip install -q \"mmpose==1.3.1\"\n",
        "\n",
        "# Install Gazelle\n",
        "%pip install -q \"gazelle-gaze==0.0.3\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "oAg1HU3Bcgqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now '**restart**' the session.\n",
        "\n",
        "To ensure all libraries are loaded correctly, we need to restart the Colab runtime.\n",
        "\n",
        "1.  Click **Runtime** > **Restart session**.\n",
        "2.  After restarting, run the setup cells (1.1, 1.2, 1.3) again.\n",
        "3.  Once the setup is complete, proceed with the rest of the notebook."
      ],
      "metadata": {
        "id": "ve9Q5fi9cioe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.4 Import Libraries\n",
        "# This cell imports all the necessary python libraries for the analysis.\n",
        "from __future__ import annotations\n",
        "import argparse, os, math, warnings, itertools\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "\n",
        "import cv2, torch, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from scipy.signal import butter, filtfilt\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score, r2_score, mean_absolute_error\n",
        "from sklearn.svm import LinearSVC, SVC, SVR\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.base import clone\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "from mmdet.apis import init_detector, inference_detector\n",
        "from mmdet.structures import DetDataSample\n",
        "from mmdet.models.trackers import ByteTracker\n",
        "from mmpose.evaluation.functional import nms\n",
        "from gazelle.model import get_gazelle_model\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg') # Use a non-interactive backend\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"✅ Libraries imported successfully.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NQN9eLq3clEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Download Models & Data**\n",
        "\n",
        "This section downloads the pre-trained models required for face detection and gaze estimation."
      ],
      "metadata": {
        "id": "ZHFClw2OcnIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.1 Download Pre-trained Models\n",
        "import gdown\n",
        "\n",
        "# --- Model Paths (Set to download from Google Drive) --- #\n",
        "DET_CONFIG_PATH = \"primateface_cascade-rcnn_r101_fpn_1x_coco.py\"\n",
        "DET_CHECKPOINT_PATH = \"best_coco_bbox_mAP_epoch_12.pth\"\n",
        "GAZELLE_CHECKPOINT_PATH = \"gazelle_dinov2_vitl14.pt\"\n",
        "\n",
        "# GDrive file IDs\n",
        "det_config_id = \"1Y_YFdIDRcWQLI-gRiCnOrDxCptzCiiNp\"\n",
        "det_checkpoint_id = \"1zZ8S31zPHX5BWYKbnHxI1QOqP-fPnVFO\"\n",
        "gazelle_checkpoint_id = \"1_wZ3V5yY4n0Z0X2Z0oQp-J0yXk8n_ZqY\" # Example ID, replace with actual\n",
        "\n",
        "# Download function\n",
        "def download_gdrive(file_id, output_name):\n",
        "    if not os.path.exists(output_name):\n",
        "        print(f\"Downloading {output_name}...\")\n",
        "        gdown.download(id=file_id, output=output_name, quiet=False)\n",
        "    else:\n",
        "        print(f\"{output_name} already exists. Skipping download.\")\n",
        "\n",
        "print(\"--- Downloading Models ---\")\n",
        "download_gdrive(det_config_id, DET_CONFIG_PATH)\n",
        "download_gdrive(det_checkpoint_id, DET_CHECKPOINT_PATH)\n",
        "download_gdrive(gazelle_checkpoint_id, GAZELLE_CHECKPOINT_PATH)\n",
        "print(\"\\n✅ All models downloaded.\")\n",
        "\n",
        "# --- Other Configurations ---\n",
        "GAZELLE_MODEL_NAME = \"gazelle_dinov2_vitl14\"\n",
        "DEFAULT_DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "DEFAULT_DET_THRESHOLD = 0.1"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tCxL_94RcpKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Load Your Video**\n",
        "Choose the video you want to analyze. The default option uses a short demo clip.\n"
      ],
      "metadata": {
        "id": "vt1wcbe4cqy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.1 Configure Video Source\n",
        "from google.colab import drive\n",
        "import gdown\n",
        "\n",
        "# --- Form Parameters ---\n",
        "data_source_option = \"Use Demo Video\"  #@param [\"Use Demo Video\", \"Use a Google Drive Link\", \"Use a Path from Mounted Google Drive\"]\n",
        "\n",
        "# #@markdown ---\n",
        "# #@markdown ### For Option 2: Provide a Google Drive Link\n",
        "# video_gdrive_link = \"PASTE_YOUR_GOOGLE_DRIVE_LINK_HERE\"  #@param {type:\"string\"}\n",
        "\n",
        "# #@markdown ---\n",
        "# #@markdown ### For Option 3: Provide a Path from Your Mounted Drive\n",
        "# mounted_drive_path = \"/content/drive/MyDrive/my_videos/gaze_video.mp4\"  #@param {type:\"string\"}\n",
        "# mount_my_drive = False  #@param {type:\"boolean\"}\n",
        "# #---------------------------------------------------------------------------------\n",
        "\n",
        "VIDEO_FILE_PATH = \"\"\n",
        "\n",
        "try:\n",
        "    if data_source_option == \"Use Demo Video\":\n",
        "        print(\"▶️ Using the demo video.\")\n",
        "        demo_video_id = \"1U-_dSnbt7KrigPhjuVPlKE_vgTpEPHQ4\" # Example ID\n",
        "        demo_video_filename = \"gaze_following_demo_video.mp4\"\n",
        "        download_gdrive(demo_video_id, demo_video_filename)\n",
        "        VIDEO_FILE_PATH = demo_video_filename\n",
        "\n",
        "    elif data_source_option == \"Use a Google Drive Link\":\n",
        "        print(\"▶️ Using a Google Drive link.\")\n",
        "        if \"PASTE_YOUR\" in video_gdrive_link:\n",
        "            raise ValueError(\"Please paste your GDrive link.\")\n",
        "        linked_video_filename = \"user_gdrive_video.mp4\"\n",
        "        gdown.download(video_gdrive_link, linked_video_filename, quiet=False, fuzzy=True)\n",
        "        VIDEO_FILE_PATH = linked_video_filename\n",
        "\n",
        "    elif data_source_option == \"Use a Path from Mounted Google Drive\":\n",
        "        print(\"▶️ Using a path from mounted Google Drive.\")\n",
        "        if mount_my_drive:\n",
        "            drive.mount('/content/drive')\n",
        "        if not Path(mounted_drive_path).exists():\n",
        "            raise FileNotFoundError(f\"Path not found: {mounted_drive_path}\")\n",
        "        VIDEO_FILE_PATH = mounted_drive_path\n",
        "\n",
        "    if not os.path.exists(VIDEO_FILE_PATH):\n",
        "        raise FileNotFoundError(f\"Final video path is invalid: {VIDEO_FILE_PATH}\")\n",
        "\n",
        "    print(f\"\\n✅ Success! Video ready for analysis:\\n- {VIDEO_FILE_PATH}\")\n",
        "\n",
        "except (ValueError, FileNotFoundError) as e:\n",
        "    print(f\"\\n❌ Error: {e}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YPWQr_PjcuMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Helper Functions & Analysis Pipeline**\n",
        "This cell contains the core logic for video processing, feature engineering, and visualization. You can run it and keep it collapsed.\n"
      ],
      "metadata": {
        "id": "6ImhfLgrcwLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.1 Define Helper Functions & Core Pipeline\n",
        "# This cell contains all the functions from the original script.\n",
        "# It is collapsed by default for clarity."
      ],
      "metadata": {
        "id": "4fMAuJgEc4Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Configure and Run Full Analysis**\n",
        "This is the main step. Adjust the parameters below and run the cell to perform the full analysis, including feature engineering, model training, and visualization."
      ],
      "metadata": {
        "id": "Rv1-knOmc7UG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5.1 Run Analysis\n",
        "#@markdown ### 1. Analysis Parameters\n",
        "#@markdown Heuristic settings for defining a gaze-follow event.\n",
        "det_thr = 0.5 #@param {type:\"slider\", min:0.1, max:0.9, step:0.05}\n",
        "turn_thresh_degrees = 20 #@param {type:\"slider\", min:5, max:45, step:5}\n",
        "horizon_frames = 15 #@param {type:\"slider\", min:5, max:30, step:1}\n",
        "\n",
        "#@markdown ### 2. Data Continuity Filter\n",
        "#@markdown Minimum consecutive frames required for an interaction to be analyzed.\n",
        "min_lookback_frames = 5 #@param {type:\"slider\", min:1, max:15, step:1}\n",
        "min_lookahead_A_frames = 5 #@param {type:\"slider\", min:1, max:15, step:1}\n",
        "\n",
        "#@markdown ### 3. Output Configuration\n",
        "output_dir = \"gaze_following_output\" #@param {type:\"string\"}\n",
        "run_training = True #@param {type:\"boolean\"}\n",
        "generate_plots = True #@param {type:\"boolean\"}\n",
        "generate_video = True #@param {type:\"boolean\"}\n",
        "\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "if not os.path.exists(VIDEO_FILE_PATH):\n",
        "    print(\"❌ Video path not set. Please run Cell 3.1 successfully.\")\n",
        "else:\n",
        "    # Create config object from parameters\n",
        "    config = {\n",
        "        \"video\": VIDEO_FILE_PATH,\n",
        "        \"output_dir\": output_dir,\n",
        "        \"device\": DEFAULT_DEVICE,\n",
        "        \"det_thr\": det_thr,\n",
        "        \"horizon\": horizon_frames,\n",
        "        \"min_lookback\": min_lookback_frames,\n",
        "        \"min_lookahead_A\": min_lookahead_A_frames\n",
        "    }\n",
        "\n",
        "    os.makedirs(config['output_dir'], exist_ok=True)\n",
        "    print(f\"Outputs will be saved to: {config['output_dir']}\")\n",
        "\n",
        "    # --- Run Pipeline ---\n",
        "    df_raw, fps = process_video(config['video'], config)\n",
        "\n",
        "    min_lookahead_B_val = max(config['horizon'], 1)\n",
        "    df_filtered = filter_continuous_segments(df_raw, config['min_lookback'], config['min_lookahead_A'], min_lookahead_B_val)\n",
        "\n",
        "    if df_filtered.empty:\n",
        "        print(\"\\n❌ No continuous interaction segments found with the current filter settings. Analysis cannot proceed.\")\n",
        "    else:\n",
        "        df_features = engineer_advanced_features(df_filtered.copy(), fps)\n",
        "        X, y = label_follow_events(df_features, fps, horizon=config['horizon'], turn_thresh=turn_thresh_degrees)\n",
        "\n",
        "        if run_training and X.shape[0] > 0 and len(np.unique(y)) > 1:\n",
        "            results = train_eval(X, y)\n",
        "            if generate_plots:\n",
        "                pr_curve_plot(results, y, os.path.join(config['output_dir'], 'pr_curve.png'))\n",
        "        elif not run_training:\n",
        "             print(\"\\nSkipping model training as requested.\")\n",
        "        else:\n",
        "            print(\"\\nSkipping model training due to insufficient data after filtering.\")\n",
        "            results = {} # Ensure results dict exists\n",
        "\n",
        "        if generate_plots:\n",
        "             plot_gaze_directionality(df_filtered, os.path.join(config['output_dir'], 'gaze_directionality.png'))\n",
        "             plot_gaze_eccentricity(df_filtered, os.path.join(config['output_dir'], 'gaze_eccentricity.png'))\n",
        "             plot_dynamics_with_follow_events(df_filtered, X, y, config['horizon'], os.path.join(config['output_dir'], 'dynamics_events.png'))\n",
        "             analyze_gaze_cross_correlation(df_features, fps, os.path.join(config['output_dir'], 'cross_correlations'))\n",
        "             # Plot a few example sequences\n",
        "             true_indices = np.where(y)[0]\n",
        "             if len(true_indices) > 0:\n",
        "                 plot_gaze_sequence([df_filtered.index[true_indices[0]]], config['video'], df_filtered, y, os.path.join(config['output_dir'], 'sequence_follow'))\n",
        "             false_indices = np.where(~y)[0]\n",
        "             if len(false_indices) > 0:\n",
        "                 plot_gaze_sequence([df_filtered.index[false_indices[0]]], config['video'], df_filtered, y, os.path.join(config['output_dir'], 'sequence_nofollow'))\n",
        "             # Attention Cloud plots\n",
        "             generate_attention_cloud(df_filtered, 0, config['video'], config['output_dir'])\n",
        "             generate_combined_attention_cloud(df_filtered, config['video'], config['output_dir'])\n",
        "\n",
        "\n",
        "        if generate_video:\n",
        "            generate_qualitative_gaze_video(config['video'], df_filtered, y, os.path.join(config['output_dir'], 'gaze_following_demo_annotated.mp4'))\n",
        "\n",
        "    print(\"\\n--- ✅ Processing Finished ---\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XMujIZNDc-WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Visualize Results**\n",
        "The plots generated in the previous step are displayed below for review.\n"
      ],
      "metadata": {
        "id": "Lf4fAVf7dAY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6.1 Display Output Figures\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Create a list of all generated PNG files\n",
        "figure_dir = Path(output_dir)\n",
        "png_files = list(figure_dir.glob('**/*.png'))\n",
        "\n",
        "if not png_files:\n",
        "    print(\"No plot images found. Please run the full analysis cell (5.1) with 'generate_plots' enabled.\")\n",
        "else:\n",
        "    print(f\"Displaying {len(png_files)} generated plots from the '{output_dir}' directory:\")\n",
        "    for img_path in sorted(png_files):\n",
        "        print(f\"\\n--- {img_path.name} ---\")\n",
        "        display(Image(filename=str(img_path), width=700))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KwEOp_q6dDJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **7. Summary and Next Steps**\n",
        "\n",
        "Congratulations! You've used a sophisticated pipeline to analyze gaze-following behavior.\n",
        "\n",
        "**Here's a recap of what we accomplished:**\n",
        "1.  We loaded a video and used a face detector with a tracker to get stable IDs for two primates.\n",
        "2.  We applied the **Gazelle** model to estimate gaze direction and uncertainty for each individual.\n",
        "3.  We trained multiple machine learning models to predict whether a \"follower\" primate would turn its head in response to a \"gazer's\" look.\n",
        "4.  We generated several visualizations to help interpret the results:\n",
        "    *   **Gaze Sequence Plots**: Frame-by-frame examples of \"follow\" and \"no-follow\" events.\n",
        "    *   **Precision-Recall Curve**: Shows the performance trade-off for the different classifiers.\n",
        "    *   **Gaze Dynamics**: Time-series plots showing how gaze direction and uncertainty change over time.\n",
        "    *   **Attention Clouds**: Heatmaps visualizing the accumulated gaze direction of each primate.\n",
        "\n",
        "**Next Steps:**\n",
        "*   Explore the `gaze_following_output` directory to find all the generated plots and the annotated demo video.\n",
        "*   Experiment with the parameters in **Cell 5.1**, such as the `turn_thresh_degrees` or `horizon_frames`, to see how they affect model performance.\n",
        "*   Adapt this notebook to run on your own two-primate videos by changing the video source in **Cell 3.1**."
      ],
      "metadata": {
        "id": "g8t63N0ZdFjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 8. Resources\n",
        "1. [PrimateFace](https://github.com/PrimateFace/PrimateFace)\n",
        "2. [Gazelle](https://gazelle-gaze.github.io/)\n",
        "3. [mmdetection](https://github.com/open-mmlab/mmdetection)"
      ],
      "metadata": {
        "id": "_eupU8PQdHAj"
      }
    }
  ]
}